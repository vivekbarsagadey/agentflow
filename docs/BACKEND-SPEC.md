**complete, enterprise-grade BACKEND-SPEC.md** for **AgentFlow Core**, matching your HLD + LLD + API Spec.

This document is designed for your backend team so they can implement the system exactly as architected.
It includes:

* Module-level specifications
* File structure
* Internal API contracts
* Node runtime engine specs
* Queue bandwidth logic
* Source plugin system
* Error framework
* Logging + metrics
* Security and environment configs

---

# ðŸ“„ **BACKEND-SPEC.md â€” AgentFlow Core Backend Specification**

**Version:** 1.0
**Audience:** Backend Engineers, Architects
**Tech:** Python 3.11+, FastAPI, LangGraph, AsyncIO

---

# -------------------------------------------------------------

# **1. Architectural Overview**

# -------------------------------------------------------------

AgentFlow Backend provides:

âœ” Workflow validation
âœ” Runtime graph building (from JSON)
âœ” Node engine execution
âœ” Source registry & connection adapters
âœ” Queue bandwidth enforcement
âœ” Storage for workflows
âœ” REST API Layer via FastAPI

Backend is modularized into **API Layer**, **Runtime Engine**, **Source System**, **Data Models**, and **Utils**.

---

# -------------------------------------------------------------

# **2. Folder Structure (Final)**

# -------------------------------------------------------------

```
backend/
â”‚
â”œâ”€â”€ agentflow_core/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ workflows.py
â”‚   â”‚   â”‚   â”œâ”€â”€ sources.py
â”‚   â”‚   â”‚   â”œâ”€â”€ queues.py
â”‚   â”‚   â”‚   â””â”€â”€ health.py
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ workflow_model.py
â”‚   â”‚   â”‚   â”œâ”€â”€ source_model.py
â”‚   â”‚   â”‚   â”œâ”€â”€ queue_model.py
â”‚   â”‚   â”‚   â””â”€â”€ exec_model.py
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â””â”€â”€ validator.py
â”‚   â”‚
â”‚   â”œâ”€â”€ runtime/
â”‚   â”‚   â”œâ”€â”€ builder.py
â”‚   â”‚   â”œâ”€â”€ executor.py
â”‚   â”‚   â”œâ”€â”€ registry.py
â”‚   â”‚   â”œâ”€â”€ rate_limiter.py
â”‚   â”‚   â””â”€â”€ state.py
â”‚   â”‚
â”‚   â”œâ”€â”€ nodes/
â”‚   â”‚   â”œâ”€â”€ input_node.py
â”‚   â”‚   â”œâ”€â”€ router_node.py
â”‚   â”‚   â”œâ”€â”€ llm_node.py
â”‚   â”‚   â”œâ”€â”€ image_node.py
â”‚   â”‚   â”œâ”€â”€ db_node.py
â”‚   â”‚   â””â”€â”€ aggregator_node.py
â”‚   â”‚
â”‚   â”œâ”€â”€ sources/
â”‚   â”‚   â”œâ”€â”€ llm_openai.py
â”‚   â”‚   â”œâ”€â”€ image_openai.py
â”‚   â”‚   â”œâ”€â”€ db_postgres.py
â”‚   â”‚   â””â”€â”€ api_http.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â”œâ”€â”€ errors.py
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â”‚
â”‚   â””â”€â”€ config/
â”‚       â”œâ”€â”€ settings.py
â”‚       â””â”€â”€ secrets.py
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ test_runtime.py
    â”œâ”€â”€ test_api.py
    â”œâ”€â”€ test_nodes.py
    â””â”€â”€ test_sources.py
```

---

# -------------------------------------------------------------

# **3. Core Modules Specification**

# -------------------------------------------------------------

---

# ============================

# **3.1 Runtime Engine**

# ============================

### Purpose:

Convert WorkflowSpec â†’ LangGraph â†’ Executable graph.

### Components:

* **builder.py**
* **executor.py**
* **registry.py**
* **rate_limiter.py**
* **state.py**

---

## **3.1.1 builder.py (Graph Builder)**

### Responsibilities:

* Convert WorkflowSpec into LangGraph `StateGraph`
* Instantiate nodes using factory router
* Register queues & sources
* Add edges & compile graph

### Methods:

#### `build_graph_from_json(spec: WorkflowSpecModel) -> CompiledGraph`

**Input:** workflow JSON
**Output:** compiled LangGraph runnable

**Internal steps:**

1. Clear old registry
2. Load sources into registry
3. Load queues with bandwidth config
4. Instantiate `StateGraph(GraphState)`
5. For each node
   â†’ call `create_node_callable()`
   â†’ `graph.add_node()`
6. Add edges
7. Compile graph
8. Return runnable

---

## **3.1.2 executor.py (Workflow Execution)**

### Method:

#### `run_workflow(spec: WorkflowSpecModel, initial_state: GraphState) -> GraphState`

Steps:

1. Validate workflow
2. Build graph
3. Execute using `.invoke()`
4. Capture:

   * Runtime metrics
   * Node trace
   * Token usage
5. Return final state

---

## **3.1.3 registry.py (Runtime Registry)**

### State:

```python
SOURCES: Dict[str, SourceModel]
QUEUES: Dict[str, QueueModel]
NODE_META: Dict[str, dict]
LAST_USAGE: Dict[str, float]
```

Purpose:

* Global lookup for nodes, queues, sources during execution.

---

## **3.1.4 rate_limiter.py (Queue Bandwidth Control)**

### Method:

#### `check_rate_limit(queue_id: str)`

Supports:

* max_messages_per_second
* max_requests_per_minute
* tokens_per_minute
* weighted subqueues

If queue is overloaded â†’ sleep or queue internally.

---

## **3.1.5 state.py (GraphState Definition)**

### Graph state struct:

```python
class GraphState(TypedDict, total=False):
    user_input: str
    intent: str
    text_result: str
    image_result: Any
    db_result: Any
    final_output: Any
    metadata: dict
```

---

# ============================

# **3.2 Node Engine**

# ============================

Each node is an executable function receiving and returning `GraphState`.

---

## **3.2.1 input_node.py**

Pass-through node.

---

## **3.2.2 router_node.py**

### Logic:

```python
if starts with "img:" â†’ intent = "image"
else â†’ intent = "text"
```

In future: LLM-based classifier.

---

## **3.2.3 llm_node.py**

### Factory:

```python
def llm_node_factory(node_id: str):
    def _node(state):
        # get source client
        # send prompt
        # write state["text_result"]
```

---

## **3.2.4 image_node.py**

### Factory:

Generates images from prompt.

---

## **3.2.5 db_node.py**

### Factory:

Executes SQL from state:

```python
query = state.get("db_query") or default
rows = db.execute(query)
```

---

## **3.2.6 aggregator_node.py**

### Combines all results:

```python
state["final_output"] = {
    "text": state.get("text_result"),
    "image": state.get("image_result"),
    "db": state.get("db_result")
}
```

---

# ============================

# **3.3 Sources System**

# ============================

### Purpose:

Dynamic plugin system for:

* LLMs
* Image models
* Databases
* HTTP APIs

---

## **3.3.1 Source Client Factory**

`get_llm_from_source(source)`
`get_image_model(source)`
`get_db_connection(source)`
`get_api_client(source)`

Source kinds:

| kind  | module          |
| ----- | --------------- |
| llm   | llm_openai.py   |
| image | image_openai.py |
| db    | db_postgres.py  |
| api   | api_http.py     |

---

# -------------------------------------------------------------

# **4. API Layer Specification**

# -------------------------------------------------------------

Already included in `API-SPEC.md`, but backend expectations:

* Requests validated using Pydantic
* Routes separated by domain (workflows, sources, queues)
* Responses follow error spec
* Workflow Execution uses `executor.run_workflow()`

---

# -------------------------------------------------------------

# **5. Error Handling**

# -------------------------------------------------------------

Unified error format:

```json
{
  "status": "error",
  "message": "Meaningful description",
  "details": "Optional stack trace"
}
```

Error classes:

```
ValidationError
ExecutionError
SourceConnectionError
QueueRateLimitExceeded
InternalServerError
```

All logged in `utils/logger.py`.

---

# -------------------------------------------------------------

# **6. Logging & Metrics**

# -------------------------------------------------------------

### Node-level logging:

* Entry timestamp
* Exit timestamp
* Latency per node
* Node failures
* Source usage

### Metrics:

* Total workflow runtime
* Tokens consumed
* Cost (if computed)
* Nodes executed trace

Logged to:

```
/log/agentflow_core.log
```

---

# -------------------------------------------------------------

# **7. Security**

# -------------------------------------------------------------

* JWT required for all endpoints
* Secrets stored in environment variables
* API keys encrypted at rest

Environment variables:

```
AGENTFLOW_DB_URL=
OPENAI_API_KEY=
JWT_SECRET=
LOG_LEVEL=INFO
```

---

# -------------------------------------------------------------

# **8. Backend Runtime Requirements**

# -------------------------------------------------------------

### Python environment:

```
python 3.11
pip install -r requirements.txt
```

### Required Libraries:

* fastapi
* langgraph
* openai
* psycopg
* pydantic
* uvicorn
* httpx

---

# -------------------------------------------------------------

# **9. Test Specification**

# -------------------------------------------------------------

### Unit Tests:

```
tests/
- test_builder.py
- test_executor.py
- test_nodes.py
- test_sources.py
```

Test coverage targets:

* 85% runtime engine
* 100% node factories
* 100% API validation

---

# -------------------------------------------------------------

# **10. Deployment Spec**

# -------------------------------------------------------------

### Container:

Dockerfile includes:

```
FROM python:3.11  
RUN pip install fastapi uvicorn ...  
CMD ["uvicorn", "agentflow_core.api.main:app", "--host", "0.0.0.0"]
```

### Load balancing:

Use Nginx or API Gateway.

### Scaling:

Stateless â†’ horizontally scalable.

---

# -------------------------------------------------------------

# âœ… BACKEND-SPEC.md IS COMPLETE

# -------------------------------------------------------------

If you want, I can now generate:

### ðŸ”¥ FRONTEND-SPEC.md

### ðŸ”¥ WORKFLOW-SCHEMA.md (JSON Schema for validation)

### ðŸ”¥ Node Plugin Architecture Document

### ðŸ”¥ Developer Checklist for Backend Team

### ðŸ”¥ Zip-ready folder with empty backend files + shell script

Just tell me which one you want next.
